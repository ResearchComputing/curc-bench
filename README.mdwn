curc-bench is a regression testing benchmark suite developed at and
for University of Colorado Boulder Research Computing. It uses
linpack, stream, and osu-micro-benchmarks.

## Commands

* `bench create`
* `bench add`
* `bench submit`
* `bench process`
* `bench reserve`
* `bench update-nodes`

## Tests available

* `--test node`: each node in the test runs a stream and
  linpack benchmark
* `--test bandwidth`: pairs of nodes selected from each switch
  run osu_bw
* `--test alltoall-pair`: pairs of nodes selected from each
  swtich run osu_alltoall
* `--test alltoall-switch`: the nodes connected to each switch
  run osu_alltoall
* `--test alltoall-rack`: the nodes in each rack run
  osu_alltoall

## Session directory

The `bench create` command creates a new "session directory" which is
used to track the state of a given benchmark run. The other commands
expect this directory to exist ahead of time.

* `node_list` (1)
* `bench.log` (2)
* `results.log` (3)
* `${test_type}/` (4)
  * `pass_nodes` (5)
  * `fail_nodes` (6)
  * `error_nodes` (7)
  * `tests/` (8)
    * `${test}/` (9)
      * `${test}.job` (10)
      * `node_list` (11)
      * `slurm-*.out` (12)
      * `${output_files}` (13)

`bench create` creates the base session directory, and generates the
root `node_list` file (1).

`bench add` consults the root `node_list` (1) to generate tests of the
requested `${test_type}` (4). (Valid types are `node`, `bandwidth`,
`alltoall-pair`, `alltoall-switch`, and `alltoall-rack`.) Each test
generates a `${test}.job` (10) Slurm job script in an individual
`${test}/` (9) directory, as well as a test-specific `node_list` file
(11).

`bench submit` submits a Slurm job for each `${test}/` (9)
directory. This directory is used as the working directory for the
job, which runs the contained `${test}.job` (10) script. Because it is
the working directory for the job, Slurm and the payload tests are
expected to write output to the `${test}/` (9) directory. (11, 12)

`bench process` inspects the `${test}/` (9) directories for payload
test output (13) to evaluate pass/fail for each test. Test results are
summarized for each `${test_type}/` (4) in `pass_nodes` (5),
`fail_nodes` (6), and `error_nodes` (7) files, which share the same
format as `node_list` (1, 10). (The `tests/` directory serves to
separate valid `${test}/` (9) directories from these summary files.)

`bench reserve` and `bench update-node` create Slurm reservations or
mark nodes down, respectively, based on the result summaries generated
by `bench process`. (4, 5, 6)

`bench.log` contains logging and debugging information about curc-bench
including Slurm job information and error messages. (2)

`results.log` is created after `bench process` is run and contains a
results summaries for all tests. (3)

## Configuration

`general.py` has default qos, account, and user information for
submitting jobs and creating reservations. This can be overridden on the
command line.

`alltoall_conf.py`, `bandwidth_conf.py`, `ior.py`, and `node_conf.py` contain
test specific configuration. This includes the path to the test executable,
modules that will be loaded in the batch script, nodes to run on with topology
information if needed, and passing test values.

## Running curc-bench at CU-Boulder Research Computing

1. Load modules and prepare the environment

   ```
   $ source /projects/rcops/cb/environment.sh
   $ bench create
   ```

2. Node tests

   ```
   $ bench add --test node
   # Submit jobs with optional arguments, PM_RES is usually month.day-shas
   $ bench submit --test node --reservation PM_RES --account ACCOUNT --qos QOS
   $ bench process --test node # after all jobs done
   $ bench reserve --test node
   ```

3. Bandwidth tests

   ```
   $ bench add --test bandwidth
   $ bench submit --test bandwidth
   $ bench process --test bandwidth --reservation PM_RES # after all jobs done
   $ bench reserve --test bandwidth
   ```

4. All-to-all tests: pairs of nodes

   ```
   $ bench add --test alltoall-pair
   $ bench submit --test alltoall-pair
   $ bench process --test alltoall-pair --reservation PM_RES # after all jobs done
   $ bench reserve --test alltoall-pair
   ```

5. All-to-all tests: switch groups

   ```
   $ bench add --test alltoall-switch
   $ bench submit --test alltoall-switch
   $ bench process --test alltoall-switch --reservation PM_RES # after all jobs done
   $ bench reserve --test alltoall-switch
   ```

6. All-to-all tests: rack groups

   ```
   $ bench add --test alltoall-rack
   $ bench submit --test alltoall-rack
   $ bench process --test alltoall-rack --reservation PM_RES # after all jobs done
   $ bench reserve --test alltoall-rack
   ```

## Running code tests

    $ python setup.py test

## Non-Python dependencies

* [IOR](http://sourceforge.net/projects/ior-sio/files/)
* [hpl](http://www.netlib.org/benchmark/hpl/)
* [osu-micro-benchmark](http://mvapich.cse.ohio-state.edu/benchmarks/)
* [stream.c v5.9](https://github.com/gregs1104/stream-scaling/blob/master/stream.c)
* Intel-provided linpack
* OpenMPI or IntelMPI

*stream.c has been tuned. If you download a fresh copy it needs to be
re-tuned.*
